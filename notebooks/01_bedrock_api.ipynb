{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0916a3a-e402-48b7-a775-ce739e4aeaf4",
   "metadata": {},
   "source": [
    "# Amazon Bedrock - API Gateway invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1fda97-9150-484a-8cfa-86ec9568fc61",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "\n",
    "We are going to invoke Amazon API Gateway through `requests`"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install -q Pillow\n",
    "%pip install -q requests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b7d5e0e486e6eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "108c611c-7246-45c4-9f1e-76888b5076eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "import requests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up API Url"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aab13c2f838ef5f"
  },
  {
   "cell_type": "code",
   "id": "01dfcb38",
   "metadata": {},
   "source": [
    "api_url = \"<API_URL>\"\n",
    "api_key = \"<API_KEY>\"\n",
    "team_id = \"<TEAM_ID>\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### List Foundation Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a05df323cf5a3937"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.get(\n",
    "    f\"{api_url}/list_foundation_models\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7980b636b83900",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Amazon Titan Express",
   "metadata": {
    "collapsed": false
   },
   "id": "da0b6adce15431a"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokenCount\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52f4863f7087b478",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc3fcb09982ca1b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Express\n",
    "\n",
    "Use Converse API"
   ],
   "id": "cf4401400d770108"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "id": "3e091fb42b54846",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "5ff1bb09d99d5a86",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan - Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdad7b84877d58f3"
  },
  {
   "cell_type": "code",
   "source": [
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c1fc6bad63a0413",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokenCount\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc0001e8ad4ee38",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b01accb866acca2"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8001d35feb35896",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan - Streaming\n",
    "\n",
    "Use Converse API"
   ],
   "id": "317afe68be7c6861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import time",
   "id": "85ba181f70a349c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "ebf05308bcbf8a5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header",
   "id": "b81cdbece05a14e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "e295fbb469db86e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Amazon Titan Embeddings G1 - Text",
   "metadata": {
    "collapsed": false
   },
   "id": "b295a7fe0daf53c3"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f27165ac44476061",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8c60a0959b67029"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    },\n",
    "    json={\"inputs\": prompt},\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"embedding\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee4adcb783809f8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Amazon Titan Text Embeddings v2",
   "id": "b9e34394ae43c89b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\"\n",
    "\n",
    "parameters = {\n",
    "    \"dimensions\": 1024,\n",
    "    \"normalize\": True\n",
    "}"
   ],
   "id": "c3aca79d34114e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header",
   "id": "a9fd6156c4b99ddf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    },\n",
    "    json={\"inputs\": prompt, \"parameters\": parameters},\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"embedding\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "3ed80ff9239c9c0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Multimodal Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6771f2302a04d386"
  },
  {
   "cell_type": "code",
   "source": [
    "import base64"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee416645da4931d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "image_path = \"./images/battery_image.png\"\n",
    "\n",
    "with open(image_path, 'rb') as image_file:\n",
    "    byte_file = base64.b64encode(image_file.read()).decode('utf-8')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e85ead6b3d5e84e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-embed-image-v1\"\n",
    "\n",
    "prompt = byte_file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2784e3fe45ec8156",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d7a7227d16d2763"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings-image\"\n",
    "    },\n",
    "    json={\"inputs\": prompt},\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"embedding\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c50847210248423b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Image Generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c50f7e0a3636e30a"
  },
  {
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3d4428c51c8eed1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-image-generator-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"taskType\": \"TEXT_IMAGE\",\n",
    "    \"imageGenerationConfig\": {\n",
    "      \"cfgScale\": 8,\n",
    "      \"seed\": 0,\n",
    "      \"quality\": \"standard\",\n",
    "      \"width\": 512,\n",
    "      \"height\": 512,\n",
    "      \"numberOfImages\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "blue backpack on a table\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35feba817d174ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `image` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6a598bb635a7e55"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"textToImageParams\": {\"text\": prompt}, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"image\"\n",
    "    }\n",
    ")\n",
    "response_body = response.json()[0][\"images\"]\n",
    "\n",
    "base_64_img_str = response_body[0]\n",
    "\n",
    "image = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c63c5d251f9b490",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 3 Sonnet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "493c5326cd335edc"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71eddddcca48a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "951ecf9ac524a7eb"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "661c42b37c7e6bb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want to add a system prompt",
   "id": "8a84460a5b16a6b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "    \"system\": \"Always translate the answer in Italian\"\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "f9fbe04588fe5549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "ed19bfb5e8d2875f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 3 Sonnet - Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f549bdeacaa3857c"
  },
  {
   "cell_type": "code",
   "source": [
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a853e4efa20e6dcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4d500887c723392",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header.\n",
    "\n",
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a2bdc6817184350"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7caa17625da04a4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Anthropic Claude 3 Sonnet - Multi-modal",
   "metadata": {
    "collapsed": false
   },
   "id": "4ffe6dc033339bfb"
  },
  {
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7e52555e820a315",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "image_path = \"./images/battery_image.png\"\n",
    "\n",
    "with open(image_path, 'rb') as image_file:\n",
    "    byte_file = base64.b64encode(image_file.read()).decode('utf-8')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f25b45e4691bd89",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'webp',\n",
    "                        \"source\": {\n",
    "                            \"bytes\": byte_file\n",
    "                        }\n",
    "                    }\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"what is in the image?\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d403e1559b125f9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header.\n",
    "\n",
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d4d35d2dd6c332d"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502d810a724d48d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 3 Sonnet - Multi-modal Streaming\n",
    "\n",
    "Use Converse API"
   ],
   "id": "ae0eaaba3976b248"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import time"
   ],
   "id": "ed71fbfb450f5ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image_path = \"./images/battery_image.png\"\n",
    "\n",
    "with open(image_path, 'rb') as image_file:\n",
    "    byte_file = base64.b64encode(image_file.read()).decode('utf-8')"
   ],
   "id": "f6f951359c89a716",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'webp',\n",
    "                        \"source\": {\n",
    "                            \"bytes\": byte_file\n",
    "                        }\n",
    "                    }\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"what is in the image?\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ],
   "id": "84cc6d31c7e9904d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header.\n",
    "\n",
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "id": "92222073b817e717"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "14a942e886664755",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 2.1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-03T19:54:36.281695Z",
     "end_time": "2023-12-03T19:54:36.473526Z"
    }
   },
   "id": "b2e573ae7da087eb"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff10e5a6755a541c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c2abd9717358791",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 2.1\n",
    "\n",
    "Use Converse API"
   ],
   "id": "61a8c9b8f34ebca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "493910a961e94fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "38dfc35b410ea261",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 2.1 - Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9089364042213d86"
  },
  {
   "cell_type": "code",
   "source": [
    "import time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed1562cc9d7d0e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0d9c3b325ba2b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dc7d6648f48be31"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f56a5198f9ba995",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 2.1 - Streaming\n",
    "\n",
    "Use Converse API"
   ],
   "id": "64d7ecd8d2fcfbb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import time",
   "id": "2a59370f6de72e2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "c91b910cfab50218",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header",
   "id": "d2bc234667e86e73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\",\n",
    "        \"streaming\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "e9ff590a1a5511a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mistral Large",
   "id": "cc0eeb8809262025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"<s>[INST]What is Amazon Bedrock?[/INST]\""
   ],
   "id": "87bb27a1577e4c9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "f52e0e34486351ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mistral Large - Streaming",
   "id": "75b1c11634a7e45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import time",
   "id": "7428fb0ffba96d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"<s>[INST]What is Amazon Bedrock?[/INST]\""
   ],
   "id": "de41f708273ebd04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header",
   "id": "22087df855fd70f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "26af837de5f5ca38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mistral Large - Streaming\n",
    "\n",
    "Converse API"
   ],
   "id": "5cdd44af3e91693f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import time",
   "id": "8a25facca5eb6291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "d115a9c401797037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header",
   "id": "3bece76083aa04b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\",\n",
    "        \"streaming\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "8b8b388700c20d83",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AI21 Jurassic Ultra"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-03T13:10:43.291944Z",
     "end_time": "2023-12-03T13:11:09.157936Z"
    }
   },
   "id": "c328c5f73d4c7bdc"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"ai21.j2-ultra\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa26c48ec1aeab16",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea79f5b1b6109f01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### AI21 Jurassic Ultra\n",
    "\n",
    "Use Converse API"
   ],
   "id": "edb19c9c31b56845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"ai21.j2-ultra\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "df35e608148c51a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "a0190eac120b5df6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cohere Command"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9752b99cfc6f5820"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"cohere.command-text-v14\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 4000,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbf825bf9bb73547",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3d9f6caa85b76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cohere Embed Multilingual"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c79d64d065f2ef36"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"cohere.embed-multilingual-v3\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"input_type\": \"search_document\"\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "Cosa è Amazon Bedrock?\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88220bf28574cae2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7399f4684358ba6"
  },
  {
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    }\n",
    ")\n",
    "text = response.json()[0][\"embedding\"]\n",
    "\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bc18a3c7831e198",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stability AI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "435a013dbbc838c8"
  },
  {
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import io\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97473b06d53d637b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"cfg_scale\": 5,\n",
    "    \"seed\": 452345,\n",
    "    \"steps\": 60,\n",
    "    \"style_preset\": \"photographic\",\n",
    "    \"clip_guidance_preset\": \"FAST_GREEN\",\n",
    "    \"sampler\": \"K_DPMPP_2S_ANCESTRAL\",\n",
    "    \"width\": 768\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "a beautiful mountain landscape\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9858c3060d1dc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `image` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe57cae96e45ccc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"text_prompts\": ([{\"text\": prompt, \"weight\": 1.0}]), \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"image\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response_body = response.json()[0][\"artifacts\"]\n",
    "\n",
    "base_64_img_str = response_body[0].get(\"base64\")\n",
    "\n",
    "image = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "image"
   ],
   "id": "aea806694bee71f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Amazon Bedrock - Custom Model",
   "id": "9be9e43e651a7104"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Amazon Titan Express",
   "id": "139a19aad259b60e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "model_arn = \"<PROVISIONED_THROUGHPUT_ARN>\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokenCount\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "id": "71c5743ac4dac1be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model` with a custom model, include the parameter `model_arn` in the query",
   "id": "e7bad34588f2a6f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}&model_arn={model_arn}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "78d1c8531d78aabc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Amazon Bedrock - Cross-Region Inference",
   "id": "f401ee88ff139841"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Amazon Anthropic Claude 3.5 Sonnet",
   "id": "21ab2466101ad148"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import time",
   "id": "97b9e4e1a73d05be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"<INFERENCE_PROFILE_ID>\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "402e6399a937c0b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header.\n",
    "\n",
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "id": "8b385edb593e50d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "start_time = time.time()\n",
    "max_time = 120\n",
    "\n",
    "while (time.time() - start_time) < max_time:\n",
    "    response = requests.post(\n",
    "        f\"{api_url}/invoke_model?model_id={model_id}&requestId={request_id}\",\n",
    "        json={},\n",
    "        headers={\n",
    "            \"x-api-key\": api_key,\n",
    "            \"team_id\": team_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"generated_text\" in response.json()[0]:\n",
    "        break\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "text = response.json()[0][\"generated_text\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "cf3535c12b85512d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Amazon SageMaker Endpoint",
   "id": "3d886c6d29c1a5e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### All MiniLM L6 v2\n",
    "\n",
    "All MiniLM L6 v2 embeddings model deployed from Amazon SageMaker JumpStart.\n",
    "\n",
    "##### Important! Refer to the specific model documentation for defining the proper inputs and parameters for the model"
   ],
   "id": "3a6d52a90eb5c24f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"<MODEL_NAME>\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"mode\": \"embedding\",\n",
    "    \"InferenceComponentName\": \"<INFERENCE_COMPONENT_NAME>\"\n",
    "}\n",
    "\n",
    "prompt = {\n",
    "    \"text_inputs\": \"Hello\"\n",
    "}"
   ],
   "id": "d99bcea92ffc2827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model` with a custom model, include the parameter `model_arn` in the query",
   "id": "7eafed2c6ccb34ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0][\"embedding\"]\n",
    "\n",
    "print(text)"
   ],
   "id": "c93bb8d43cf0f19d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Meta Llama-3 - Messages API\n",
    "\n",
    "Fine-tuned Meta Llama-3 model hosted in Amazon SageMaker through Hugging Face TGI container.\n",
    "\n",
    "##### Important! Refer to the specific model documentation for defining the proper inputs and parameters for the model"
   ],
   "id": "b6c2184e89059295"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"<MODEL_NAME>\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"model\": \"placeholder-model\", # placeholder, needed\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.2,\n",
    "    \"return_full_text\": True,\n",
    "    \"stop\": [],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": \"Hello\"\n",
    "        }\n",
    "    ]"
   ],
   "id": "13c54a644fe432b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model` with a custom model, include the parameter `model_arn` in the query",
   "id": "8bbae954859a1059"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = requests.post(\n",
    "    f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    json={\"inputs\": prompt, \"parameters\": model_kwargs},\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    }\n",
    ")\n",
    "\n",
    "text = response.json()[0]\n",
    "\n",
    "print(text)"
   ],
   "id": "a4708f62a8f0d967",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "35bda9518a60a0a5"
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
