{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0916a3a-e402-48b7-a775-ce739e4aeaf4",
   "metadata": {},
   "source": [
    "# Amazon Bedrock - API Gateway invocation with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install requirements"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13e47ddb1a21d4e3"
  },
  {
   "cell_type": "code",
   "source": "%pip install -q langchain==0.2.1",
   "metadata": {
    "collapsed": false
   },
   "id": "61680c3370f0fdfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c1fda97-9150-484a-8cfa-86ec9568fc61",
   "metadata": {},
   "source": [
    "### Setup Environment\n",
    "\n",
    "We are going to invoke Amazon API Gateway through `langchain`"
   ]
  },
  {
   "cell_type": "code",
   "id": "01dfcb38",
   "metadata": {},
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setting up API Url"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab69a6fe95aa53c0"
  },
  {
   "cell_type": "code",
   "source": [
    "api_url = \"<API_URL>\"\n",
    "api_key = \"<API_KEY>\"\n",
    "team_id = \"<TEAM_ID>\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72fdd11d2c738200",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Default Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3c4f7d46a8f6cae"
  },
  {
   "cell_type": "code",
   "source": [
    "PROMPT_DEFAULT = PromptTemplate(\n",
    "    template=\"{question}\", input_variables=[\"question\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af780098f703dae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92b211ebdfec003f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AmazonAPIGateway class Extended\n",
    "\n",
    "This is an example of the AmazonAPIGateway class extended for handling both `invoke_model` and `invoke_model_with_response_stream` with long-polling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c89230195ffe44f"
  },
  {
   "cell_type": "code",
   "source": [
    "import ast\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.amazon_api_gateway import AmazonAPIGateway\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "def convert_to_dict_list(input):\n",
    "    try:\n",
    "        if isinstance(input, str):\n",
    "            result = ast.literal_eval(input)\n",
    "            if isinstance(result, list) and all(isinstance(item, dict) for item in result):\n",
    "                return result\n",
    "            else:\n",
    "                return None\n",
    "        elif isinstance(input, list):\n",
    "            return input\n",
    "        else:\n",
    "            raise ValueError\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "'''\n",
    "This is an utility function for converting prompts from Claude < 3 to the new messages API.\n",
    "Prerequisites:\n",
    "- Make sure you are putting your system prompt in the tags <system> and </system>\n",
    "- If you have a conversation history, make sure you are putting it in the tags <history> and </history>\n",
    "\n",
    "This function will work also if you are directly providing messages in an array format\n",
    "'''\n",
    "def convert_prompt_to_messages(prompt, model_kwargs=dict()):\n",
    "    messages = convert_to_dict_list(prompt)\n",
    "\n",
    "    if messages is None:\n",
    "        # Find the content between <system> and </system> tags\n",
    "        system_content_match = re.search(r'<system>(.*?)</system>', prompt, re.DOTALL)\n",
    "\n",
    "        if system_content_match:\n",
    "            # Extract the content between the tags\n",
    "            system_prompt = system_content_match.group(1)\n",
    "\n",
    "            # Remove the <system> and </system> tags and their content from the original text\n",
    "            prompt = re.sub(r'<system>.*?</system>', '', prompt, flags=re.DOTALL)\n",
    "        else:\n",
    "            system_prompt = None\n",
    "\n",
    "        prompt = prompt.replace(\"<history>\", \"\")\n",
    "        prompt = prompt.replace(\"</history>\", \"\")\n",
    "\n",
    "        messages = []\n",
    "        role_regex = re.compile(r'(Human:|Assistant:)\\s?(.*?)(?=Human:|Assistant:|$)', re.DOTALL)\n",
    "\n",
    "        for match in role_regex.finditer(prompt):\n",
    "            role, content = match.groups()\n",
    "            role = role.strip(':').lower()\n",
    "            if role == \"human\" or role == \"Human\":\n",
    "                role = \"user\"\n",
    "            else:\n",
    "                role = \"assistant\"\n",
    "            messages.append({\"role\": role, \"content\": [{\"text\": content.strip()}]})\n",
    "    else:\n",
    "        if len(messages) > 0:\n",
    "            if len(messages) == 1:\n",
    "                system_prompt = None\n",
    "            else:\n",
    "                if messages[0][\"role\"] == \"user\" and messages[1][\"role\"] != \"assistant\":\n",
    "                    system_prompt = messages.pop(0)[\"content\"]\n",
    "                else:\n",
    "                    system_prompt = None\n",
    "        else:\n",
    "            system_prompt = None\n",
    "\n",
    "    if system_prompt is not None:\n",
    "        model_kwargs[\"system\"] = system_prompt\n",
    "        \n",
    "    if messages and messages[-1]['role'] == 'assistant' and not messages[-1]['content'][0]['text']:\n",
    "        messages.pop()\n",
    "\n",
    "    return messages, model_kwargs\n",
    "\n",
    "class AmazonAPIGatewayExtended(AmazonAPIGateway):\n",
    "    streaming: bool = False\n",
    "    polling_wait: int = 2\n",
    "    max_time: int = 120\n",
    "    chat: bool = False\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call out to Amazon API Gateway model.\n",
    "\n",
    "                Args:\n",
    "                    prompt: The prompt to pass into the model.\n",
    "                    stop: Optional list of stop words to use when generating.\n",
    "\n",
    "                Returns:\n",
    "                    The string generated by the model.\n",
    "\n",
    "                Example:\n",
    "                    .. code-block:: python\n",
    "\n",
    "                        response = se(\"Tell me a joke.\")\n",
    "                \"\"\"\n",
    "\n",
    "        if self.chat:\n",
    "            _model_kwargs = self.model_kwargs or {}\n",
    "            messages, _model_kwargs = convert_prompt_to_messages(prompt, _model_kwargs)\n",
    "\n",
    "            payload = {\n",
    "                \"inputs\": messages,\n",
    "                \"parameters\": _model_kwargs\n",
    "            }\n",
    "        else:\n",
    "            _model_kwargs = self.model_kwargs or {}\n",
    "            payload = self.content_handler.transform_input(prompt, _model_kwargs)\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "            )\n",
    "            if not self.streaming:\n",
    "                text = self.content_handler.transform_output(response)\n",
    "            else:\n",
    "                request_id = response.json()[0][\"request_id\"]\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                while (time.time() - start_time) < self.max_time:\n",
    "                    response = requests.post(\n",
    "                        self.api_url + f\"&requestId={request_id}\",\n",
    "                        headers=self.headers,\n",
    "                        json={},\n",
    "                    )\n",
    "\n",
    "                    if \"generated_text\" in response.json()[0]:\n",
    "                        break\n",
    "\n",
    "                    time.sleep(self.polling_wait)\n",
    "\n",
    "                text = self.content_handler.transform_output(response)\n",
    "\n",
    "        except Exception as error:\n",
    "            raise ValueError(f\"Error raised by the service: {error}\")\n",
    "\n",
    "        if stop is not None:\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(text)\n",
    "\n",
    "        return text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "817707bade3cbf16",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AmazonAPIGateway class for Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1036496d63494f7b"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "class AmazonAPIGatewayEmbeddings(Embeddings):\n",
    "    def __init__(self, api_url, headers):\n",
    "        self.api_url = api_url\n",
    "        self.headers = headers\n",
    "\n",
    "    def embed_documents(self, texts: List[str], parameters: dict = {}) -> List[List[float]]:\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\"inputs\": text, \"parameters\": parameters},\n",
    "                headers=self.headers\n",
    "            )\n",
    "            results.append(response.json()[0][\"embedding\"])\n",
    "\n",
    "        return results\n",
    "\n",
    "    def embed_query(self, text: str, parameters: dict = {}) -> List[float]:\n",
    "        response = requests.post(\n",
    "                self.api_url,\n",
    "                json={\"inputs\": text, \"parameters\": parameters},\n",
    "                headers=self.headers\n",
    "            )\n",
    "\n",
    "        return response.json()[0][\"embedding\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a27359cebc8e4d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4fb28cb23f0938a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Text Express"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af55aa6df24ce87"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokenCount\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4888c89f7e8d863a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    },\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "894261019a22dada",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36733de76c2b9fdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Text Express\n",
    "\n",
    "Use Converse API"
   ],
   "id": "185a2d03b34ad65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "id": "81da502233a43dd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    chat=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "310e0482e208a1b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "id": "655d7870f03a8d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Text Express - Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "422ef37d26e01785"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokenCount\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93930cd31f347094",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46bf3e37f93e03a0"
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57789a4c56e97299",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44f8797b69688edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Text Express - Streaming\n",
    "\n",
    "Use Converse API"
   ],
   "id": "e91e247784becf17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "id": "f3d8f2b165bb1e28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header",
   "id": "e3e8cc117bb6a7fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True,\n",
    "    chat=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "575bf1a2374961db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "id": "8389ef8a76138166",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Amazon Titan Embeddings G1 - Text",
   "metadata": {
    "collapsed": false
   },
   "id": "b67287e907524296"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "997cc2c52a738511",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bd2e2fd4cedbf61"
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings = AmazonAPIGatewayEmbeddings(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c4cad5523cca6be",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings.embed_query(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d49575c6ab14cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Amazon Titan Text Embeddings v2",
   "id": "4e5835828426ae87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"dimensions\": 1024,\n",
    "    \"normalize\": True\n",
    "}"
   ],
   "id": "3f145c96e14ccb02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header",
   "id": "20ae7beb0898c4c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embeddings = AmazonAPIGatewayEmbeddings(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    }\n",
    ")"
   ],
   "id": "e8f6552ed04029bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embeddings.embed_query(prompt, parameters)",
   "id": "feb42f53bfac1b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Titan Mulitmodal Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445996dd3ddd2a91"
  },
  {
   "cell_type": "code",
   "source": [
    "import base64"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2485fcfade5e830b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "image_path = \"./images/img513074217-1493907994177.webp\"\n",
    "\n",
    "with open(image_path, 'rb') as image_file:\n",
    "    byte_file = base64.b64encode(image_file.read()).decode('utf-8')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb566364afde94af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-embed-image-v1\"\n",
    "\n",
    "prompt = byte_file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34cd396c2a838177",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b14d3aacd0f4979"
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings = AmazonAPIGatewayEmbeddings(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings-image\"\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4761932a28cfee11",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings.embed_query(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f69b76ad20845e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 3 Sonnet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7623feedc19c4aa"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "656c4eab7620b4d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c27974c97962430d"
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    chat=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "951df9a784cf356d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3078c288dc1658b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Claude 3 Sonnet - Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f607c0466df1fff2"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4e65333760ddeb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header.\n",
    "\n",
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2541395482f4b233"
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True,\n",
    "    chat=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff6acf743eadf93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1989781b75440c09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want to add a system prompt",
   "id": "99af2ccbf8dd5ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "    \"system\": \"Always translate the answer in Italian\"\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'user', 'content': [{\"text\": \"What is Amazon Bedrock?\"}]}\n",
    "]"
   ],
   "id": "512ce1744e531a7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True,\n",
    "    chat=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "5ef36c36036a853a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "id": "3ace06841761a614",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Claude 3 Sonnet - Multi-modal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8494ede25e7f5630"
  },
  {
   "cell_type": "code",
   "source": [
    "import base64"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6880bb6c97f2ed7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "image_path = \"./images/img513074217-1493907994177.webp\"\n",
    "\n",
    "with open(image_path, 'rb') as image_file:\n",
    "    byte_file = base64.b64encode(image_file.read()).decode('utf-8')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76c297968bdd9153",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "}\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'webp',\n",
    "                        \"source\": {\n",
    "                            \"bytes\": byte_file\n",
    "                        }\n",
    "                    }\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"what is in the image?\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c18c13f20b5d345",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` with Messages API, include the parameter `messages_api` as `True` or `true` in the header.\n",
    "\n",
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f38c4fc8b801689c"
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\",\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True,\n",
    "    chat=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6ebb180ec5488f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = llm._call(prompt=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9ed6fb6880dbac",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Anthropic Claude 2.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad25b48d295b156c"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb892ce59028717b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    },\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69271a4728ef7f4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df7cf8ac8c02f927",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Amazon Claude 2.1 - Streaming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5428d9c66080c520"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be0b4f235421a3c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f061897927fe4c5"
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"streaming\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deae745496603b43",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b4e8e4947444517",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Amazon Claude 2.1 - Streaming\n",
    "\n",
    "Use Converse API"
   ],
   "id": "5be20913191ad07f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"anthropic.claude-v2:1\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9,\n",
    "    \"stopSequences\": [\"\\n\\nHuman:\"]\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "id": "9de4acc3264c2f33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model_with_response_stream` with long-polling, include the parameter `streaming` as `True` or `true` in the header",
   "id": "e855b2577d6a7e43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\",\n",
    "        \"streaming\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    chat=True,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "caf65d8b3acf5b03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "id": "74dad69b58a6e3c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AI21 Jurassic"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c120a2882b020fc"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"ai21.j2-ultra\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa1052aefb20cdfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    },\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac5ed8eaa47db4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b03cae309f168acd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### AI21 Jurassic\n",
    "\n",
    "Use Converse API"
   ],
   "id": "f465849595459838"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"ai21.j2-ultra\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokens\": 4096,\n",
    "    \"temperature\": 0.2,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "prompt = [{\"role\": \"user\", \"content\": [{\"text\": \"What is Amazon Bedrock?\"}]}]"
   ],
   "id": "ce514a3469878f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"messages_api\": \"true\"\n",
    "    },\n",
    "    model_kwargs=model_kwargs,\n",
    "    chat=True\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "729c3d9dfa5982f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "id": "3b3ddd85506e9ba7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cohere Command"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a858937e5bb2323"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"cohere.command-text-v14\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens\": 4000,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3da9d7bc0845280",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    },\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79fca035ad40626a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de2485ded8b16703",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cohere Embed Multilingual"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa4a402897715c47"
  },
  {
   "cell_type": "code",
   "source": [
    "model_id = \"cohere.embed-multilingual-v3\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"input_type\": \"search_document\"\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is Amazon Bedrock?\"\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae28e9e6b3ca430a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "For using Bedrock boto3 `invoke_model` for generating embeddings, include the parameter `type` as `embeddings` in the header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ade96f23cbd03dda"
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings = AmazonAPIGatewayEmbeddings(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id,\n",
    "        \"type\": \"embeddings\"\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b554627f4c8dc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "embeddings.embed_query(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed2c4a6599edcaa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Amazon Bedrock - Custom Model",
   "id": "68ae658779604ac1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Amazon Titan Text Express",
   "id": "ab58c42d9d78c4e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "model_arn = \"<PROVISIONED_THROUGHPUT_ARN>\"\n",
    "\n",
    "model_kwargs = {\n",
    "    \"maxTokenCount\": 4096,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "prompt = \"What is Amazon Bedrock?\""
   ],
   "id": "c0ef9dd8eb281e6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For using Bedrock boto3 `invoke_model` with a custom model, include the parameter `model_arn` in the query",
   "id": "17dd8fdd110bdbe7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "llm = AmazonAPIGatewayExtended(\n",
    "    api_url=f\"{api_url}/invoke_model?model_id={model_id}&model_arn={model_arn}\",\n",
    "    headers={\n",
    "        \"x-api-key\": api_key,\n",
    "        \"team_id\": team_id\n",
    "    },\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT_DEFAULT,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "2e59b5d982cecbbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.predict(question=prompt)\n",
    "\n",
    "# Print response\n",
    "print(response)"
   ],
   "id": "a8cc7f5348467ba3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
